// Алгоритм перевода десятичной дроби в двоичную
// Умножаем дробь на 2.

// Записываем целую часть (0 или 1) — это очередной бит.

// Оставляем дробную часть и повторяем процесс, пока не достигнем нужной точности или пока дробная часть не станет нулём.

console.log(`
5.

1) Исходные вероятности символов:
    a: 0.10
    b: 0.15
    c: 0.05
    d: 0.50
    e: 0.10
    f: 0.10
    
2) Интервалы символов:
    a: [0.00, 0.10)
    b: [0.10, 0.25)
    c: [0.25, 0.30)
    d: [0.30, 0.80)
    e: [0.80, 0.90)
    f: [0.90, 1.00)

3) Кодирование строки "acefdb":
    Шаг 1 (a): Интервал [0.00, 0.10).
    Шаг 2 (c): Интервал [0.25, 0.30) внутри [0.00, 0.10) → [0.025, 0.030).
    Шаг 3 (e): Интервал [0.80, 0.90) внутри [0.025, 0.030) → [0.029, 0.0295).
    Шаг 4 (f): Интервал [0.90, 1.00) внутри [0.029, 0.0295) → [0.02945, 0.0295).
    Шаг 5 (d): Интервал [0.30, 0.80) внутри [0.02945, 0.0295) → [0.029465, 0.02949).
    Шаг 6 (b): Интервал [0.10, 0.25) внутри [0.029465, 0.02949) → [0.0294695, 0.0294765).

4) Выбор числа внутри финального интервала:
    Среднее значение: (0.0294695 + 0.0294765) / 2 ≈ 0.029473.
    Преобразование в двоичную строку:
    Число 0.029473 в двоичной системе: 0.000001111000101...
    Применяем к числу 0.029472875:

    

    Шаг	            Действие	        Результат	Целая часть (бит)
    1	0.029472875 × 2 = 0.05894575	0.05894575	    0
    2	0.05894575 × 2 = 0.1178915	    0.1178915	    0
    3	0.1178915 × 2 = 0.235783	    0.235783	    0
    4	0.235783 × 2 = 0.471566	        0.471566	    0
    5	0.471566 × 2 = 0.943132	        0.943132	    0
    6	0.943132 × 2 = 1.886264	        0.886264	    1
    7	0.886264 × 2 = 1.772528	        0.772528	    1
    8	0.772528 × 2 = 1.545056	        0.545056	    1
    9	0.545056 × 2 = 1.090112	        0.090112	    1
    10	0.090112 × 2 = 0.180224	        0.180224	    0
    11	0.180224 × 2 = 0.360448	        0.360448	    0
    12	0.360448 × 2 = 0.720896	        0.720896	    0
    13	0.720896 × 2 = 1.441792	        0.441792	    1
    Получаем двоичную дробь:
    0.0000011110001... (первые 13 битов).

    Мы должны выбрать минимальное количество битов, чтобы число гарантированно попадало в финальный интервал [low, high).

    Правило:
    Двоичная запись должна однозначно определять интервал. То есть:

    Любое число, начинающееся с выбранных битов, должно лежать в [low, high).

    Если отбросить последний бит, интервал станет слишком широким.

    Пример:
    Для интервала [0.0294695, 0.02947625) двоичное представление 0.0000011110001 гарантирует, что любое число с таким началом попадает в интервал.

    Проверка:

    Нижняя граница (low = 0.0294695) в двоичном виде начинается как 0.000001111000....

    Верхняя граница (high = 0.02947625) — 0.0000011110001....

    Если взять 0.0000011110001, оно точно попадает в [low, high).

        Округляем до минимального количества бит, покрывающих интервал: 0000011110001.


    `)
